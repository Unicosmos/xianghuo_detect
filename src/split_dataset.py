#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†åˆ‡åˆ†è„šæœ¬

åŸºäº ultralytics.data.split æ¨¡å—å®ç°çš„é€šç”¨æ•°æ®é›†åˆ‡åˆ†å·¥å…·
æ”¯æŒç›®æ ‡æ£€æµ‹å’Œåˆ†ç±»æ•°æ®é›†çš„åˆ‡åˆ†

ä½œè€…: AI Assistant
åˆ›å»ºæ—¶é—´: 2025-01-27
"""

import argparse
import os
import random
import shutil
import sys
import yaml
from pathlib import Path
from typing import List, Tuple, Union

try:
    from ultralytics.data.split import autosplit, split_classify_dataset
    from ultralytics.utils import LOGGER
except ImportError:
    print("é”™è¯¯: æ— æ³•å¯¼å…¥ ultralytics åº“ï¼Œè¯·ç¡®ä¿å·²å®‰è£… ultralytics")
    print("å®‰è£…å‘½ä»¤: pip install ultralytics")
    sys.exit(1)


def detect_dataset_type(dataset_path: Path) -> str:
    """
    æ£€æµ‹æ•°æ®é›†ç±»å‹
    
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        
    Returns:
        æ•°æ®é›†ç±»å‹: 'detection' æˆ– 'classification'
    """
    dataset_path = Path(dataset_path)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ images å’Œ labels ç›®å½• (ç›®æ ‡æ£€æµ‹æ ¼å¼)
    if (dataset_path / "images").exists() and (dataset_path / "labels").exists():
        return "detection"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªå­ç›®å½•ï¼Œæ¯ä¸ªå­ç›®å½•åŒ…å«å›¾ç‰‡ (åˆ†ç±»æ ¼å¼)
    subdirs = [d for d in dataset_path.iterdir() if d.is_dir()]
    if len(subdirs) > 1:
        # æ£€æŸ¥å­ç›®å½•æ˜¯å¦åŒ…å«å›¾ç‰‡æ–‡ä»¶
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        for subdir in subdirs[:3]:  # æ£€æŸ¥å‰3ä¸ªå­ç›®å½•
            files = list(subdir.iterdir())
            if any(f.suffix.lower() in image_extensions for f in files):
                return "classification"
    
    # é»˜è®¤è¿”å›æ£€æµ‹ç±»å‹
    return "detection"


def split_detection_dataset(
    dataset_path: Union[str, Path],
    output_path: Union[str, Path] = None,
    train_ratio: float = 0.8,
    val_ratio: float = 0.2,
    test_ratio: float = 0.0,
    annotated_only: bool = True,
    copy_files: bool = True
) -> Path:
    """
    åˆ‡åˆ†ç›®æ ‡æ£€æµ‹æ•°æ®é›†
    
    Args:
        dataset_path: åŸå§‹æ•°æ®é›†è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™åœ¨åŸè·¯å¾„åŸºç¡€ä¸Šæ·»åŠ  '_split' åç¼€
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        annotated_only: æ˜¯å¦åªåŒ…å«æœ‰æ ‡æ³¨çš„å›¾ç‰‡
        copy_files: æ˜¯å¦å¤åˆ¶æ–‡ä»¶ï¼ŒFalse åˆ™åªç”Ÿæˆæ–‡ä»¶åˆ—è¡¨
        
    Returns:
        åˆ‡åˆ†åçš„æ•°æ®é›†è·¯å¾„
    """
    dataset_path = Path(dataset_path)
    
    if output_path is None:
        # æ„é€ åŒ…å«æ¯”ä¾‹çš„ç›®å½•åç§°
        ratio_str = f"{int(train_ratio*100)}-{int(val_ratio*100)}"
        if test_ratio > 0:
            ratio_str += f"-{int(test_ratio*100)}"
        output_path = dataset_path.parent / f"{dataset_path.name}_split_{ratio_str}"
    else:
        output_path = Path(output_path)
    
    # æ£€æŸ¥æ¯”ä¾‹æ˜¯å¦åˆç†
    total_ratio = train_ratio + val_ratio + test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        raise ValueError(f"æ¯”ä¾‹æ€»å’Œå¿…é¡»ä¸º 1.0ï¼Œå½“å‰ä¸º {total_ratio}")
    
    images_dir = dataset_path / "images"
    labels_dir = dataset_path / "labels"
    
    if not images_dir.exists():
        raise FileNotFoundError(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {images_dir}")
    
    # ä½¿ç”¨ ultralytics çš„ autosplit åŠŸèƒ½ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨
    LOGGER.info(f"å¼€å§‹åˆ‡åˆ†ç›®æ ‡æ£€æµ‹æ•°æ®é›†: {dataset_path}")
    
    # è®¾ç½®æƒé‡
    weights = (train_ratio, val_ratio, test_ratio)
    
    # ä½¿ç”¨ autosplit ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨
    autosplit(
        path=images_dir,
        weights=weights,
        annotated_only=annotated_only
    )
    
    if copy_files:
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        output_path.mkdir(parents=True, exist_ok=True)
        
        splits = ['train', 'val', 'test']
        split_files = [
            images_dir.parent / "autosplit_train.txt",
            images_dir.parent / "autosplit_val.txt", 
            images_dir.parent / "autosplit_test.txt"
        ]
        
        for split, split_file in zip(splits, split_files):
            if not split_file.exists():
                continue
                
            # åˆ›å»ºç›®å½•
            split_images_dir = output_path / split / "images"
            split_labels_dir = output_path / split / "labels"
            split_images_dir.mkdir(parents=True, exist_ok=True)
            split_labels_dir.mkdir(parents=True, exist_ok=True)
            
            # è¯»å–æ–‡ä»¶åˆ—è¡¨å¹¶å¤åˆ¶æ–‡ä»¶
            with open(split_file, 'r', encoding='utf-8') as f:
                for line in f:
                    img_path = line.strip()
                    if img_path.startswith('./'):
                        img_path = img_path[2:]
                    
                    src_img = dataset_path / img_path
                    if src_img.exists():
                        # å¤åˆ¶å›¾ç‰‡
                        dst_img = split_images_dir / src_img.name
                        shutil.copy2(src_img, dst_img)
                        
                        # å¤åˆ¶å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶
                        label_name = src_img.stem + '.txt'
                        src_label = labels_dir / label_name
                        if src_label.exists():
                            dst_label = split_labels_dir / label_name
                            shutil.copy2(src_label, dst_label)
        
        # å¤åˆ¶å…¶ä»–æ–‡ä»¶
        for file_name in ['classes.txt', 'notes.json']:
            src_file = dataset_path / file_name
            if src_file.exists():
                shutil.copy2(src_file, output_path / file_name)
        
        # ç”Ÿæˆæ–°çš„ data.yaml æ–‡ä»¶
        generate_data_yaml(
            output_path=output_path,
            dataset_type="detection",
            train_ratio=train_ratio,
            val_ratio=val_ratio,
            test_ratio=test_ratio,
            original_dataset_path=dataset_path
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for split_file in split_files:
            if split_file.exists():
                split_file.unlink()
    
    LOGGER.info(f"æ•°æ®é›†åˆ‡åˆ†å®Œæˆ: {output_path}")
    return output_path


def split_classification_dataset_wrapper(
    dataset_path: Union[str, Path],
    output_path: Union[str, Path] = None,
    train_ratio: float = 0.8
) -> Path:
    """
    åˆ‡åˆ†åˆ†ç±»æ•°æ®é›†çš„åŒ…è£…å‡½æ•°
    
    Args:
        dataset_path: åŸå§‹æ•°æ®é›†è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        
    Returns:
        åˆ‡åˆ†åçš„æ•°æ®é›†è·¯å¾„
    """
    if output_path is not None:
        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œéœ€è¦å…ˆå¤åˆ¶æ•°æ®é›†
        dataset_path = Path(dataset_path)
        output_path = Path(output_path)
        temp_path = output_path.parent / f"{dataset_path.name}_temp"
        
        if temp_path.exists():
            shutil.rmtree(temp_path)
        shutil.copytree(dataset_path, temp_path)
        
        # ä½¿ç”¨ ultralytics çš„å‡½æ•°åˆ‡åˆ†
        result_path = split_classify_dataset(temp_path, train_ratio)
        
        # é‡å‘½ååˆ°ç›®æ ‡è·¯å¾„
        if output_path.exists():
            shutil.rmtree(output_path)
        result_path.rename(output_path)
        
        # ç”Ÿæˆ data.yaml æ–‡ä»¶
        generate_data_yaml(
            output_path=output_path,
            dataset_type="classification",
            train_ratio=train_ratio,
            val_ratio=1-train_ratio,
            test_ratio=0.0,
            original_dataset_path=dataset_path
        )
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if temp_path.exists():
            shutil.rmtree(temp_path)
            
        return output_path
    else:
        # ç›´æ¥ä½¿ç”¨ ultralytics çš„å‡½æ•°
        result_path = split_classify_dataset(dataset_path, train_ratio)
        
        # ç”Ÿæˆ data.yaml æ–‡ä»¶
        generate_data_yaml(
            output_path=result_path,
            dataset_type="classification",
            train_ratio=train_ratio,
            val_ratio=1-train_ratio,
            test_ratio=0.0,
            original_dataset_path=dataset_path
        )
        
        return result_path


def count_dataset_files(dataset_path: Path, dataset_type: str) -> dict:
    """
    ç»Ÿè®¡æ•°æ®é›†æ–‡ä»¶æ•°é‡
    
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        dataset_type: æ•°æ®é›†ç±»å‹
        
    Returns:
        æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
    """
    stats = {}
    
    if dataset_type == "detection":
        images_dir = dataset_path / "images"
        labels_dir = dataset_path / "labels"
        
        if images_dir.exists():
            image_files = list(images_dir.glob("*.*"))
            image_files = [f for f in image_files if f.suffix.lower() in 
                          {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}]
            stats['images'] = len(image_files)
        else:
            stats['images'] = 0
            
        if labels_dir.exists():
            label_files = list(labels_dir.glob("*.txt"))
            stats['labels'] = len(label_files)
        else:
            stats['labels'] = 0
            
    elif dataset_type == "classification":
        class_dirs = [d for d in dataset_path.iterdir() if d.is_dir()]
        stats['classes'] = len(class_dirs)
        stats['images'] = 0
        
        for class_dir in class_dirs:
            image_files = list(class_dir.glob("*.*"))
            image_files = [f for f in image_files if f.suffix.lower() in 
                          {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}]
            stats['images'] += len(image_files)
    
    return stats


def generate_data_yaml(
    output_path: Path,
    dataset_type: str,
    train_ratio: float,
    val_ratio: float,
    test_ratio: float,
    original_dataset_path: Path = None
) -> None:
    """
    ç”Ÿæˆ data.yaml é…ç½®æ–‡ä»¶
    
    Args:
        output_path: åˆ‡åˆ†åçš„æ•°æ®é›†è·¯å¾„
        dataset_type: æ•°æ®é›†ç±»å‹
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        original_dataset_path: åŸå§‹æ•°æ®é›†è·¯å¾„
    """
    data_yaml_path = output_path / "data.yaml"
    
    # åŸºæœ¬é…ç½®
    config = {
        "path": str(output_path.absolute()),
        "train": "train/images" if dataset_type == "detection" else "train",
        "val": "val/images" if dataset_type == "detection" else "val"
    }
    
    # å¦‚æœæœ‰æµ‹è¯•é›†ï¼Œæ·»åŠ æµ‹è¯•é›†é…ç½®
    if test_ratio > 0 and (output_path / "test").exists():
        config["test"] = "test/images" if dataset_type == "detection" else "test"
    
    # è·å–ç±»åˆ«ä¿¡æ¯
    if dataset_type == "detection":
        # å°è¯•ä»åŸå§‹æ•°æ®é›†è¯»å–ç±»åˆ«ä¿¡æ¯
        classes = []
        
        # é¦–å…ˆå°è¯•ä»åŸå§‹æ•°æ®é›†çš„ data.yaml è¯»å–
        if original_dataset_path and (original_dataset_path / "data.yaml").exists():
            try:
                with open(original_dataset_path / "data.yaml", 'r', encoding='utf-8') as f:
                    original_config = yaml.safe_load(f)
                    if 'names' in original_config:
                        if isinstance(original_config['names'], dict):
                            classes = list(original_config['names'].values())
                        elif isinstance(original_config['names'], list):
                            classes = original_config['names']
            except Exception as e:
                LOGGER.warning(f"æ— æ³•è¯»å–åŸå§‹ data.yaml: {e}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä» classes.txt è¯»å–
        if not classes:
            classes_file = original_dataset_path / "classes.txt" if original_dataset_path else output_path / "classes.txt"
            if classes_file.exists():
                try:
                    with open(classes_file, 'r', encoding='utf-8') as f:
                        classes = [line.strip() for line in f if line.strip()]
                except Exception as e:
                    LOGGER.warning(f"æ— æ³•è¯»å– classes.txt: {e}")
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«
        if not classes:
            classes = ["object"]
        
        # è®¾ç½®ç±»åˆ«ä¿¡æ¯
        config["nc"] = len(classes)
        config["names"] = {i: name for i, name in enumerate(classes)}
        
    elif dataset_type == "classification":
        # å¯¹äºåˆ†ç±»æ•°æ®é›†ï¼Œä»è®­ç»ƒç›®å½•è·å–ç±»åˆ«
        train_dir = output_path / "train"
        if train_dir.exists():
            class_dirs = [d.name for d in train_dir.iterdir() if d.is_dir()]
            class_dirs.sort()  # ç¡®ä¿é¡ºåºä¸€è‡´
            config["nc"] = len(class_dirs)
            config["names"] = {i: name for i, name in enumerate(class_dirs)}
        else:
            config["nc"] = 1
            config["names"] = {0: "unknown"}
    
    # å†™å…¥ data.yaml æ–‡ä»¶
    try:
        with open(data_yaml_path, 'w', encoding='utf-8') as f:
            # æ·»åŠ æ³¨é‡Š
            f.write(f"# Ultralytics YOLO ğŸš€, AGPL-3.0 license\n")
            f.write(f"# Dataset configuration for {output_path.name}\n")
            f.write(f"# Generated automatically by split_dataset.py\n")
            f.write(f"# Split ratios - train: {train_ratio:.1%}, val: {val_ratio:.1%}")
            if test_ratio > 0:
                f.write(f", test: {test_ratio:.1%}")
            f.write(f"\n\n")
            
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
        
        LOGGER.info(f"å·²ç”Ÿæˆ data.yaml é…ç½®æ–‡ä»¶: {data_yaml_path}")
        
    except Exception as e:
        LOGGER.error(f"ç”Ÿæˆ data.yaml å¤±è´¥: {e}")


def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(
        description="æ•°æ®é›†åˆ‡åˆ†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åˆ‡åˆ†ç›®æ ‡æ£€æµ‹æ•°æ®é›† (é»˜è®¤ 8:2 æ¯”ä¾‹)
  python split_dataset.py /path/to/detection/dataset
  
  # åˆ‡åˆ†åˆ†ç±»æ•°æ®é›† (æŒ‡å®šæ¯”ä¾‹ 7:3)
  python split_dataset.py /path/to/classification/dataset --split-ratio 0.7,0.3
  
  # æŒ‡å®šè¾“å‡ºè·¯å¾„
  python split_dataset.py /path/to/dataset --output /path/to/output
  
  # åŒ…å«æµ‹è¯•é›†çš„ä¸‰åˆ†å‰² (7:2:1)
  python split_dataset.py /path/to/dataset --split-ratio 0.7,0.2,0.1
        """
    )
    
    parser.add_argument(
        "dataset_path",
        type=str,
        help="æ•°æ®é›†è·¯å¾„"
    )
    
    parser.add_argument(
        "--output", "-o",
        type=str,
        default=None,
        help="è¾“å‡ºè·¯å¾„ (é»˜è®¤åœ¨åŸè·¯å¾„åŸºç¡€ä¸Šæ·»åŠ  '_split' åç¼€)"
    )
    
    parser.add_argument(
        "--split-ratio",
        type=str,
        default="0.8,0.2",
        help="æ•°æ®é›†åˆ‡åˆ†æ¯”ä¾‹ï¼Œç”¨é€—å·åˆ†éš”ã€‚å¦‚æœåªæœ‰ä¸€ä¸ªé€—å·ï¼Œè¡¨ç¤ºtrain_ratio,val_ratioï¼›å¦‚æœæœ‰ä¸¤ä¸ªé€—å·ï¼Œè¡¨ç¤ºtrain_ratio,val_ratio,test_ratio (é»˜è®¤: 0.8,0.2)"
    )
    
    parser.add_argument(
        "--dataset-type",
        choices=["detection", "classification", "auto"],
        default="auto",
        help="æ•°æ®é›†ç±»å‹ (é»˜è®¤: auto è‡ªåŠ¨æ£€æµ‹)"
    )
    
    parser.add_argument(
        "--annotated-only",
        action="store_true",
        help="ä»…åŒ…å«æœ‰æ ‡æ³¨çš„å›¾ç‰‡ (ä»…å¯¹ç›®æ ‡æ£€æµ‹æ•°æ®é›†æœ‰æ•ˆ)"
    )
    
    parser.add_argument(
        "--no-copy",
        action="store_true",
        help="ä¸å¤åˆ¶æ–‡ä»¶ï¼Œä»…ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨ (ä»…å¯¹ç›®æ ‡æ£€æµ‹æ•°æ®é›†æœ‰æ•ˆ)"
    )
    
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­ (é»˜è®¤: 42)"
    )
    
    args = parser.parse_args()
    
    # è§£æåˆ‡åˆ†æ¯”ä¾‹
    try:
        ratios = [float(x.strip()) for x in args.split_ratio.split(',')]
        if len(ratios) == 2:
            train_ratio, val_ratio = ratios
            test_ratio = 0.0
        elif len(ratios) == 3:
            train_ratio, val_ratio, test_ratio = ratios
        else:
            print(f"é”™è¯¯: åˆ‡åˆ†æ¯”ä¾‹æ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸º 'train,val' æˆ– 'train,val,test'ï¼Œå½“å‰ä¸º: {args.split_ratio}")
            sys.exit(1)
        
        # æ£€æŸ¥æ¯”ä¾‹æ˜¯å¦åˆç†
        total_ratio = train_ratio + val_ratio + test_ratio
        if abs(total_ratio - 1.0) > 1e-6:
            print(f"é”™è¯¯: æ¯”ä¾‹æ€»å’Œå¿…é¡»ä¸º 1.0ï¼Œå½“å‰ä¸º {total_ratio}")
            sys.exit(1)
            
        if any(r < 0 for r in ratios):
            print(f"é”™è¯¯: æ¯”ä¾‹ä¸èƒ½ä¸ºè´Ÿæ•°")
            sys.exit(1)
            
    except ValueError as e:
        print(f"é”™è¯¯: æ— æ³•è§£æåˆ‡åˆ†æ¯”ä¾‹ '{args.split_ratio}': {e}")
        sys.exit(1)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    
    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
    dataset_path = Path(args.dataset_path)
    if not dataset_path.exists():
        print(f"é”™è¯¯: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        sys.exit(1)
    
    # æ£€æµ‹æ•°æ®é›†ç±»å‹
    if args.dataset_type == "auto":
        dataset_type = detect_dataset_type(dataset_path)
        print(f"è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹: {dataset_type}")
    else:
        dataset_type = args.dataset_type
    
    # ç»Ÿè®¡åŸå§‹æ•°æ®é›†ä¿¡æ¯
    stats = count_dataset_files(dataset_path, dataset_type)
    print(f"\nåŸå§‹æ•°æ®é›†ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    try:
        if dataset_type == "detection":
            # åˆ‡åˆ†ç›®æ ‡æ£€æµ‹æ•°æ®é›†
            output_path = split_detection_dataset(
                dataset_path=dataset_path,
                output_path=args.output,
                train_ratio=train_ratio,
                val_ratio=val_ratio,
                test_ratio=test_ratio,
                annotated_only=args.annotated_only,
                copy_files=not args.no_copy
            )
        elif dataset_type == "classification":
            # åˆ‡åˆ†åˆ†ç±»æ•°æ®é›†
            if test_ratio > 0:
                print("è­¦å‘Š: åˆ†ç±»æ•°æ®é›†åˆ‡åˆ†æš‚ä¸æ”¯æŒæµ‹è¯•é›†ï¼Œå°†å¿½ç•¥æµ‹è¯•é›†æ¯”ä¾‹")
            
            output_path = split_classification_dataset_wrapper(
                dataset_path=dataset_path,
                output_path=args.output,
                train_ratio=train_ratio
            )
        else:
            print(f"é”™è¯¯: ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {dataset_type}")
            sys.exit(1)
        
        print(f"\nâœ… æ•°æ®é›†åˆ‡åˆ†å®Œæˆ!")
        print(f"è¾“å‡ºè·¯å¾„: {output_path}")
        
        # ç»Ÿè®¡åˆ‡åˆ†åçš„æ•°æ®é›†ä¿¡æ¯
        if output_path.exists():
            print(f"\nåˆ‡åˆ†åæ•°æ®é›†ç»“æ„:")
            for split_dir in ['train', 'val', 'test']:
                split_path = output_path / split_dir
                if split_path.exists():
                    if dataset_type == "detection":
                        images_count = len(list((split_path / "images").glob("*.*")))
                        labels_count = len(list((split_path / "labels").glob("*.txt")))
                        print(f"  {split_dir}: {images_count} å›¾ç‰‡, {labels_count} æ ‡ç­¾")
                    elif dataset_type == "classification":
                        total_images = 0
                        class_dirs = [d for d in split_path.iterdir() if d.is_dir()]
                        for class_dir in class_dirs:
                            images = list(class_dir.glob("*.*"))
                            total_images += len(images)
                        print(f"  {split_dir}: {total_images} å›¾ç‰‡, {len(class_dirs)} ç±»åˆ«")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()